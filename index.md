---
layout: splash
permalink: /
excerpt: "Shared Task on Reproducibility of Human Evaluations in NLG
<br /> Results meeting @ INLG'21, Aberdeen, UK, September 2021"
layout: single
classes: wide
header:
  overlay_color: "#000"
  overlay_filter: "0.1"
  overlay_image: /assets/images/color-2174065_1280.png
---

## ReproGen

### Background

Across NLP, a growing body of work is looking at the issue of reproducibility. However, replicability of human evaluation experiments and reproducibility of their results is currently under-addressed, and this is of particular concern for NLG where human evaluations are the norm. With the ReproGen shared task on reproducibility of human evaluations in NLG we aim (i) to shed light on the extent to which past NLG evaluations have been replicable and reproducible, and (ii) to draw conclusions regarding how evaluations can be designed and reported to increase reproducibility. If the task is run over several years, we hope to be able to document an overall increase in levels of reproducibility over time.

### Human Evaluations to be Reproduced

With this call we invite authors of papers describing a human evaluation experiment and reporting results from it to put their paper up for reproduction in the main ReproGen Shared Task track (see below). We are also asking NLG researchers to nominate papers they themselves would like to reproduce. (Self-)nominated human evaluations should fulfill the following criteria (Intended to ensure a low barrier to participation):

* The original evaluation evaluates automatically generated textual outputs (of any length) 
* Authors are able to make the complete set of system outputs from the evaluation freely available to ReproGen participants
* Evaluators were not paid for participation
* There were between 10 and 50 evaluators
* The original evaluation involves easily accessible evaluators such as university colleagues or students

From the (self-)nominations received, we will make a selection aimed at achieving balance across NLG tasks and publication years.

Please submit your (self-)nominated evaluation papers by email to <reprogen.task@gmail.com>, attaching a PDF of the paper and explaining in the text of the email (a) which evaluation(s) reported in the paper you are nominating, and (b) why they would make good targets for reproduction.

### Tracks:

* Main Reproducibility Track: For a shared set of selected human evaluations, participants attempt to reproduce their results, using published information plus extra details provided by the authors, and making common-sense assumptions where information is still incomplete.

* RYO Track: Reproduce Your Own previous human evaluation results, and report what happened. Unshared task.


### Important Dates:

tba. tenative dates:

* 27 Jan 2021: Announcement and Call for Human Evaluations to be Reproduced
* 4 Feb 2021: First Call for Participation and registration opens
* 15 Aug 2021: Submission deadline for reproduction reports
* End September 2021: Results presented at INLG (dates to be confirmed)

### Bibliography
ReproGen [proposal](https://www.aclweb.org/anthology/2020.inlg-1.29/) at INLG'2020

### Contact Information

<reprogen.task@gmail.com>

